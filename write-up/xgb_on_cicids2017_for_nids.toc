\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Chapter 1: Introduction}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Background}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Problem Statement}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Research Questions}{3}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Research Objectives}{3}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Scope of the Dissertation}{3}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Significance of the Research}{4}{subsection.1.6}%
\contentsline {subsection}{\numberline {1.7}Dissertation Outline}{4}{subsection.1.7}%
\contentsline {section}{\numberline {2}Chapter 2: Literature Review}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}Introduction to Network Intrusion Detection System}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}How NIDS detect threats}{5}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Signature-Based Detection}{5}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Anomaly-Based Detection}{6}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Hybrid Detection}{7}{subsubsection.2.2.3}%
\contentsline {subsection}{\numberline {2.3}How NIDS are deployed in the real-world}{7}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Software and Hardware to boost performance}{8}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Rule and Model Management}{9}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Integration with SIEM/SOAR}{9}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Datasets and Evaluation}{9}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Contemporary Challenges}{10}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Encryption and Protocol Evolution}{10}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Performance at Line Rate}{10}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Data and Concept Drift}{10}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}Adversarial Robustness}{11}{subsubsection.2.5.4}%
\contentsline {subsubsection}{\numberline {2.5.5}Explainability and Analyst Trust}{11}{subsubsection.2.5.5}%
\contentsline {subsubsection}{\numberline {2.5.6}Operational Diffculties}{11}{subsubsection.2.5.6}%
\contentsline {subsection}{\numberline {2.6}Future Directions}{11}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}ML in Network Intrusion Detection Systems}{12}{subsection.2.7}%
\contentsline {subsubsection}{\numberline {2.7.1}Introduction}{12}{subsubsection.2.7.1}%
\contentsline {subsection}{\numberline {2.8}Machine Learning Approaches in NIDS}{12}{subsection.2.8}%
\contentsline {subsubsection}{\numberline {2.8.1}Supervised learning}{12}{subsubsection.2.8.1}%
\contentsline {subparagraph}{Strengths and Limitations}{13}{subparagraph*.1}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{13}{paragraph*.2}%
\contentsline {subparagraph}{Application in NIDS}{13}{subparagraph*.3}%
\contentsline {subparagraph}{Strengths and Limitations}{13}{subparagraph*.4}%
\contentsline {paragraph}{Autoencoders}{13}{paragraph*.5}%
\contentsline {subparagraph}{Application in NIDS}{13}{subparagraph*.6}%
\contentsline {subparagraph}{Strengths and Limitations}{14}{subparagraph*.7}%
\contentsline {paragraph}{Variational Autoencoders (VAEs)}{14}{paragraph*.8}%
\contentsline {subparagraph}{Application in NIDS}{14}{subparagraph*.9}%
\contentsline {subparagraph}{Strengths and Limitations}{14}{subparagraph*.10}%
\contentsline {subparagraph}{Strengths and Limitations}{14}{subparagraph*.11}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs) and their Variants (LSTM, GRU)}{14}{paragraph*.12}%
\contentsline {subparagraph}{Application in NIDS}{15}{subparagraph*.13}%
\contentsline {subparagraph}{Strengths and Limitations}{15}{subparagraph*.14}%
\contentsline {paragraph}{Autoencoders (AEs)}{15}{paragraph*.15}%
\contentsline {subparagraph}{Application in NIDS}{15}{subparagraph*.16}%
\contentsline {subparagraph}{Strengths and Limitations}{15}{subparagraph*.17}%
\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{16}{paragraph*.18}%
\contentsline {subparagraph}{Application in NIDS}{16}{subparagraph*.19}%
\contentsline {subparagraph}{Strengths and Limitations}{16}{subparagraph*.20}%
\contentsline {subsubsection}{\numberline {2.8.2}Semi-Supervised Learning}{16}{subsubsection.2.8.2}%
\contentsline {subparagraph}{Application in NIDS}{16}{subparagraph*.21}%
\contentsline {subparagraph}{Strengths and Limitations}{17}{subparagraph*.22}%
\contentsline {subsection}{\numberline {2.9}Challenges and Future Directions of ML in NIDS}{17}{subsection.2.9}%
\contentsline {subsubsection}{\numberline {2.9.1}Data Imbalance}{17}{subsubsection.2.9.1}%
\contentsline {subparagraph}{Mitigation Strategies}{17}{subparagraph*.23}%
\contentsline {subsection}{\numberline {2.10}Exploratory Data Analysis (EDA) of the CIC-IDS2017 Dataset}{18}{subsection.2.10}%
\contentsline {subsubsection}{\numberline {2.10.1}Brief Introduction}{18}{subsubsection.2.10.1}%
\contentsline {subsubsection}{\numberline {2.10.2}The CIC-IDS2017 Dataset: Background and Key Characteristics}{18}{subsubsection.2.10.2}%
\contentsline {subsubsection}{\numberline {2.10.3}Structure and Features}{20}{subsubsection.2.10.3}%
\contentsline {subsubsection}{\numberline {2.10.4}Dataset Overview}{21}{subsubsection.2.10.4}%
\contentsline {subsection}{\numberline {2.11}Preliminary EDA: Initial Impressions and Red Flags}{27}{subsection.2.11}%
\contentsline {subsection}{\numberline {2.12}Dataset Summary Statistics}{28}{subsection.2.12}%
\contentsline {subsection}{\numberline {2.13}Dataset Characteristics and Features}{29}{subsection.2.13}%
\contentsline {subsection}{\numberline {2.14}Initial Data Exploration (EDA)}{30}{subsection.2.14}%
\contentsline {subsubsection}{\numberline {2.14.1}Loading and Merging Data}{30}{subsubsection.2.14.1}%
\contentsline {subsubsection}{\numberline {2.14.2}Data Types and Initial Statistics}{30}{subsubsection.2.14.2}%
\contentsline {subsubsection}{\numberline {2.14.3}NaN (Missing Values) Treatment}{31}{subsubsection.2.14.3}%
\contentsline {subsubsection}{\numberline {2.14.4}Inf (Infinite Values) Treatment}{32}{subsubsection.2.14.4}%
\contentsline {subsubsection}{\numberline {2.14.5}Encoding Categorical Features}{32}{subsubsection.2.14.5}%
\contentsline {subsubsection}{\numberline {2.14.6}Analysis of Class Distribution}{33}{subsubsection.2.14.6}%
\contentsline {subsubsection}{\numberline {2.14.7}Feature-Wise Analysis and Outlier Detection}{33}{subsubsection.2.14.7}%
\contentsline {subsubsection}{\numberline {2.14.8}Correlation Analysis}{34}{subsubsection.2.14.8}%
\contentsline {subsubsection}{\numberline {2.14.9}Identified Challenges and their Implications for the Methodology}{35}{subsubsection.2.14.9}%
\contentsline {paragraph}{Severe Class Imbalance}{35}{paragraph*.37}%
\contentsline {paragraph}{Missing and Infinite Values}{35}{paragraph*.38}%
\contentsline {paragraph}{Redundant and Highly Correlated Features}{36}{paragraph*.39}%
\contentsline {paragraph}{Presence of Outliers}{36}{paragraph*.40}%
\contentsline {paragraph}{Categorical Feature Encoding}{36}{paragraph*.41}%
\contentsline {subsubsection}{\numberline {2.14.10}Addressing EDA Insights in the Methodology}{37}{subsubsection.2.14.10}%
\contentsline {subsubsection}{\numberline {2.14.11}Conclusion of Dataset Exploration}{38}{subsubsection.2.14.11}%
\contentsline {subsection}{\numberline {2.15}Deep Dive into Ensemble Learning}{39}{subsection.2.15}%
\contentsline {subsubsection}{\numberline {2.15.1}Introduction}{39}{subsubsection.2.15.1}%
\contentsline {subsubsection}{\numberline {2.15.2}The Basics: Foundations of Ensemble Learning}{40}{subsubsection.2.15.2}%
\contentsline {subsubsection}{\numberline {2.15.3}Bias-Variance Trade-off}{40}{subsubsection.2.15.3}%
\contentsline {subsubsection}{\numberline {2.15.4}Bagging (Bootstrap Aggregating)}{41}{subsubsection.2.15.4}%
\contentsline {subsubsection}{\numberline {2.15.5}Bootstrapping}{41}{subsubsection.2.15.5}%
\contentsline {subsubsection}{\numberline {2.15.6}Workings of Bagging}{41}{subsubsection.2.15.6}%
\contentsline {subsubsection}{\numberline {2.15.7}Random Forest (RF)}{42}{subsubsection.2.15.7}%
\contentsline {paragraph}{A Closer Look}{42}{paragraph*.42}%
\contentsline {paragraph}{RF in the NIDS: Strengths}{42}{paragraph*.43}%
\contentsline {paragraph}{Limitations}{43}{paragraph*.44}%
\contentsline {paragraph}{Applications and Impact in NIDS}{43}{paragraph*.45}%
\contentsline {subsubsection}{\numberline {2.15.8}Boosting}{43}{subsubsection.2.15.8}%
\contentsline {subsubsection}{\numberline {2.15.9}The General Principle of Gradient Boosting Machines (GBM)}{44}{subsubsection.2.15.9}%
\contentsline {subsubsection}{\numberline {2.15.10}Extreme Gradient Boosting (XGBoost)}{44}{subsubsection.2.15.10}%
\contentsline {paragraph}{Inside the Black Box}{44}{paragraph*.46}%
\contentsline {paragraph}{Key Advantages}{45}{paragraph*.47}%
\contentsline {subsubsection}{\numberline {2.15.11}Computational Complexity and Resource Requirements}{46}{subsubsection.2.15.11}%
\contentsline {subsubsection}{\numberline {2.15.12}Model Complexity and Interpretability}{46}{subsubsection.2.15.12}%
\contentsline {subsubsection}{\numberline {2.15.13}Integration with Existing Infrastructure}{46}{subsubsection.2.15.13}%
\contentsline {subsubsection}{\numberline {2.15.14}Overfitting with Improper Tuning}{46}{subsubsection.2.15.14}%
\contentsline {subsubsection}{\numberline {2.15.15}Maintaining Performance with Evolving Threats}{47}{subsubsection.2.15.15}%
\contentsline {subsubsection}{\numberline {2.15.16}Dataset Specificity and Transferability}{47}{subsubsection.2.15.16}%
\contentsline {subsubsection}{\numberline {2.15.17}Hyperparameter Tuning and Configuration}{47}{subsubsection.2.15.17}%
\contentsline {subsubsection}{\numberline {2.15.18}Increased Computational Cost and Training Time}{47}{subsubsection.2.15.18}%
\contentsline {subsubsection}{\numberline {2.15.19}Reduced Interpretability (Black-Box Nature)}{48}{subsubsection.2.15.19}%
\contentsline {subsubsection}{\numberline {2.15.20}Hyperparameter Complexity}{48}{subsubsection.2.15.20}%
\contentsline {subsubsection}{\numberline {2.15.21}Vulnerability to Adversarial Attacks}{48}{subsubsection.2.15.21}%
\contentsline {subsection}{\numberline {2.16}Conclusion}{49}{subsection.2.16}%
\contentsline {subsection}{\numberline {2.17}Deep Dive into XGBoost: Principles, Advantages, and Hyperparameters}{49}{subsection.2.17}%
\contentsline {subsubsection}{\numberline {2.17.1}Introduction to XGBoost}{49}{subsubsection.2.17.1}%
\contentsline {subsubsection}{\numberline {2.17.2}Core Principles of XGBoost}{50}{subsubsection.2.17.2}%
\contentsline {subsubsection}{\numberline {2.17.3}Ensemble Learning and Boosting}{50}{subsubsection.2.17.3}%
\contentsline {subsubsection}{\numberline {2.17.4}Gradient Boosting Machines (GBM)}{51}{subsubsection.2.17.4}%
\contentsline {subsubsection}{\numberline {2.17.5}Decision Trees as Weak Learners}{51}{subsubsection.2.17.5}%
\contentsline {subsubsection}{\numberline {2.17.6}XGBoost: Algorithmic Enhancements and Mechanics}{52}{subsubsection.2.17.6}%
\contentsline {subsubsection}{\numberline {2.17.7}Regularization in the Objective Function}{52}{subsubsection.2.17.7}%
\contentsline {subsubsection}{\numberline {2.17.8}Sparsity-Aware Split Finding}{53}{subsubsection.2.17.8}%
\contentsline {subsubsection}{\numberline {2.17.9}Approximate Greedy Algorithm and Weighted Quantile Sketch}{53}{subsubsection.2.17.9}%
\contentsline {subsubsection}{\numberline {2.17.10}Shrinkage (Learning Rate)}{53}{subsubsection.2.17.10}%
\contentsline {subsubsection}{\numberline {2.17.11}Column Subsampling (Feature Bagging)}{54}{subsubsection.2.17.11}%
\contentsline {subsubsection}{\numberline {2.17.12}Tree Pruning}{54}{subsubsection.2.17.12}%
\contentsline {subsubsection}{\numberline {2.17.13}Parallel Computing}{54}{subsubsection.2.17.13}%
\contentsline {subsubsection}{\numberline {2.17.14}Advantages of XGBoost}{55}{subsubsection.2.17.14}%
\contentsline {subsubsection}{\numberline {2.17.15}Superior Accuracy}{55}{subsubsection.2.17.15}%
\contentsline {subsubsection}{\numberline {2.17.16}High Speed and Scalability}{55}{subsubsection.2.17.16}%
\contentsline {subsubsection}{\numberline {2.17.17}Robustness Against Overfitting}{55}{subsubsection.2.17.17}%
\contentsline {subsubsection}{\numberline {2.17.18}Effective Handling of Missing Values}{56}{subsubsection.2.17.18}%
\contentsline {subsubsection}{\numberline {2.17.19}Flexibility and Customization}{56}{subsubsection.2.17.19}%
\contentsline {subsubsection}{\numberline {2.17.20}Built-in Feature Importance}{56}{subsubsection.2.17.20}%
\contentsline {subsubsection}{\numberline {2.17.21}Cross-Platform Compatibility}{57}{subsubsection.2.17.21}%
\contentsline {subsubsection}{\numberline {2.17.22}Key Hyperparameters and Their Impact}{57}{subsubsection.2.17.22}%
\contentsline {subsubsection}{\numberline {2.17.23}General Parameters}{57}{subsubsection.2.17.23}%
\contentsline {subsubsection}{\numberline {2.17.24}Booster Parameters (Tree-Specific Parameters for gbtree )}{58}{subsubsection.2.17.24}%
\contentsline {subsubsection}{\numberline {2.17.25}Learning Task Parameters}{60}{subsubsection.2.17.25}%
\contentsline {subsubsection}{\numberline {2.17.26}Tuning Strategy Implications for NIDS}{61}{subsubsection.2.17.26}%
\contentsline {subsubsection}{\numberline {2.17.27}XGBoost in Network Intrusion Detection (NIDS)}{62}{subsubsection.2.17.27}%
\contentsline {subsection}{\numberline {2.18}Identification of Research Gaps}{62}{subsection.2.18}%
\contentsline {subsubsection}{\numberline {2.18.1}Pinpointing Specific Research Gaps for Contribution}{63}{subsubsection.2.18.1}%
\contentsline {subsubsection}{\numberline {2.18.2}Novel Pre-processing Techniques, particularly Adaptive Imputation for Multi-Class}{63}{subsubsection.2.18.2}%
\contentsline {subsubsection}{\numberline {2.18.3}Optimized XGBoost Hyperparameter Tuning with Multi-Class Imbalance in Mind}{64}{subsubsection.2.18.3}%
\contentsline {section}{\numberline {3}Chapter 3: Methodology}{67}{section.3}%
\contentsline {subsection}{\numberline {3.1}Introduction}{67}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Dataset Description: CIC-IDS2017}{67}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Data Pre-processing}{67}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Data Loading and Initial Inspection}{67}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Handling Missing and Infinite Values}{68}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Handling Categorical Features}{68}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Feature Scaling/Normalization}{68}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Feature Engineering (if applicable)}{68}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Handling Class Imbalance}{69}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Model Selection}{69}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Experimental Setup}{69}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Data Splitting}{69}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Evaluation Metrics}{70}{subsubsection.3.11.2}%
\contentsline {subsection}{\numberline {3.12}Tools and Environment}{70}{subsection.3.12}%
\contentsline {subsection}{\numberline {3.13}Hyperparameter Tuning}{70}{subsection.3.13}%
\contentsline {subsection}{\numberline {3.14}Comparative Analysis}{71}{subsection.3.14}%
\contentsline {section}{\numberline {4}Chapter 4: Results}{72}{section.4}%
\contentsline {subsection}{\numberline {4.1}Introduction}{72}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Dataset Characteristics after Pre-processing}{72}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Baseline Model Performance}{74}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Hyperparameter Tuning Results}{74}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Optimized XGBoost Model Performance}{75}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Comparative Analysis Results}{79}{subsection.4.6}%
\contentsline {section}{\numberline {5}Chapter 5: Discussion}{80}{section.5}%
\contentsline {subsection}{\numberline {5.1}Interpretation of Results}{80}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Answering Research Questions}{81}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Comparison with Related Work}{81}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Limitations of the Study}{82}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Implications and Contributions}{82}{subsection.5.5}%
\contentsline {section}{\numberline {6}Chapter 6: Conclusion and Future Work}{84}{section.6}%
\contentsline {subsection}{\numberline {6.1}Conclusion}{84}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Future Work}{84}{subsection.6.2}%
\contentsline {section}{\numberline {7}Ethics}{86}{section.7}%
\contentsline {subsection}{\numberline {7.1}Ethical Statement}{86}{subsection.7.1}%
\contentsline {section}{\numberline {8}Appendix}{IX}{section.8}%
\contentsline {subsection}{\numberline {8.1}Source Code Repository}{IX}{subsection.8.1}%
