    # STEP 1: IMPORT LIBRARIES
    import pandas as pd
    import numpy as np
    import glob
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.model_selection import train_test_split, RandomizedSearchCV
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.impute import SimpleImputer, IterativeImputer # For advanced imputation (RQ1)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, precision_recall_fscore_support
    from sklearn.ensemble import RandomForestClassifier # For comparative analysis (RQ3)
    import xgboost as xgb # For core model (RQ1, RQ2, RQ3)
    import lightgbm as lgb # For comparative analysis (RQ3)
    import catboost as cb # For comparative analysis (RQ3)
    import shap # For interpretability
    from imblearn.over_sampling import SMOTE, ADASYN # For multi-class oversampling strategies (RQ1)
    from imblearn.combine import SMOTETomek # For robust oversampling + undersampling (RQ1)
    from imblearn.pipeline import Pipeline as ImbPipeline # For robust pre-processing pipeline
    from sklearn.feature_selection import SelectFromModel # For feature selection (RQ2)
    import joblib
    import warnings
    warnings.filterwarnings('ignore') # Suppress warnings for cleaner output, but be cautious in real analysis

    print("Libraries imported successfully! üéâ")

    # ---
    # Phase 2 & 3: Data Acquisition and Pre-processing & Model Development & Experimentation
    # This section covers loading, initial cleaning, advanced pre-processing,
    # data splitting, imbalance handling, feature selection, and model training/tuning.

    # ---
    # STEP 2: LOAD & INITIAL CLEAN DATA (Phase 2, aligns with RQ1)
    # This step focuses on loading all CSVs and performing initial, critical cleaning
    # to prepare for more advanced pre-processing strategies (aligns with RQ1).

    def load_and_initial_clean_data(data_path):
        """
        Loads all CIC-IDS2017 CSV files from a specified path,
        performs initial data cleaning (dropping irrelevant columns,
        standardizing missing/infinite values, converting to numeric types),
        and retains multi-class labels.
        """
        print(f"\n--- STEP 2: Loading & Initial Data Cleaning (Phase 2, RQ1) ---")
        print(f"Loading data from: {data_path}...")
        file_paths = glob.glob(f'MachineLearningCVE/*.csv')
        if not file_paths:
            raise ValueError(f"No CSV files found in {MachineLearningCSV}. Please check the path.")

        df_list = []
        for f_path in file_paths:
            try:
                df_list.append(pd.read_csv(f_path, low_memory=False))
            except Exception as e:
                print(f"Error loading {f_path}: {e}")
                continue

        if not df_list:
            raise ValueError("No dataframes were loaded successfully. Exiting.")

        df = pd.concat(df_list, ignore_index=True)
        print(f"Initial dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.")

        # Drop irrelevant columns as identified in literature (e.g., specific IPs, Flow ID, Timestamp)
        # Keeping 'Timestamp' might be useful for temporal feature engineering but dropped for now to simplify flow-based features.
        drop_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'SimillarHTTP', 'Unnamed: 11']
        df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors='ignore')
        print(f"Dropped irrelevant columns: {', '.join([c for c in drop_cols if c in df.columns])}. Shape: {df.shape}")

        # Standardize various representations of missing/infinite values to np.nan
        df.replace(['Infinity', 'NaN', np.inf, -np.inf], np.nan, inplace=True)
        # Convert all columns to numeric, coercing errors to NaN. This is aggressive but handles mixed types.
        # More granular type conversion for specific columns could be done if needed.
        df = df.apply(pd.to_numeric, errors='coerce')
        print("Standardized missing/infinite values and coerced types.")

        # Remove rows that have become all NaN after coercion (e.g., rows with only non-numeric data that couldn't be dropped)
        initial_rows_after_numeric = df.shape[0]
        df.dropna(how='all', inplace=True)
        print(f"Removed {initial_rows_after_numeric - df.shape[0]} rows that were entirely NaN. Remaining rows: {df.shape[0]}")


        # Clean column names (e.g., remove leading/trailing spaces, special characters)
        df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('/', '_').str.replace('-', '_')
        print("Cleaned column names.")

        # Handle potentially problematic column names if they contain single quotes (can happen with CIC-IDS2017)
        df.columns = df.columns.str.replace("'", "")

        # Ensure 'Label' column exists and is handled correctly as the target
        if 'Label' not in df.columns:
            raise ValueError("'Label' column not found in the dataset. Please check data source.")

        # Remove duplicate rows
        initial_rows = df.shape[0]
        df.drop_duplicates(inplace=True)
        print(f"Removed {initial_rows - df.shape[0]} duplicate rows. Remaining rows: {df.shape[0]}")

        # Rename specific attack labels for consistency and clarity (RQ2 justification for fine-grained attacks)
        # Convert to uppercase for consistency after renaming
        df['Label'] = df['Label'].replace({
            'DoS GoldenEye': 'DoS_GoldenEye',
            'DoS Hulk': 'DoS_Hulk',
            'DoS Slowhttptest': 'DoS_Slowhttptest',
            'DoS slowloris': 'DoS_Slowloris',
            'Heartbleed': 'Heartbleed',
            'Web Attack ‚Äì Brute Force': 'Web_Attack_Brute_Force',
            'Web Attack ‚Äì XSS': 'Web_Attack_XSS',
            'Web Attack ‚Äì Sql Injection': 'Web_Attack_SQL_Injection',
            'PortScan': 'PortScan',
            'DDoS': 'DDoS',
            'FTP-Patator': 'BruteForce_FTP',
            'SSH-Patator': 'BruteForce_SSH',
            'Infiltration': 'Infiltration',
            'Bot': 'Botnet'
        }).str.upper() # Apply .str.upper() after all replacements


        print(f"Unique labels (after renaming): {df['Label'].unique()}")

        # Drop constant or near-constant features BEFORE imputation
        # These features have no variance and provide no information
        constant_features = df.columns[df.nunique() <= 1]
        df.drop(columns=constant_features, inplace=True, errors='ignore')
        print(f"Dropped constant or near-constant features: {list(constant_features)}. Shape: {df.shape}")

        return df

    # ---
    # STEP 3: PRE-PROCESSING PIPELINE (Imputation, Scaling, Encoding) (Phase 2, aligns with RQ1)
    # This step builds a robust pipeline for handling missing values and scaling numerical features.
    # It also includes Label Encoding for the multi-class target variable.

    def preprocess_data(df, random_state=42):
        """
        Applies imputation, scaling, and encodes categorical features.
        Retains multi-class labels for the target variable.
        """
        print("\n--- STEP 3: Pre-processing Pipeline (Phase 2, RQ1) ---")
        print("Starting pre-processing pipeline (Imputation, Scaling, Encoding)...")

        X = df.drop('Label', axis=1)
        y = df['Label']

        # --- Handle Categorical Features (Non-Target) ---
        # Identify non-numeric columns remaining after initial cleaning
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns
        if not categorical_cols.empty:
            # For CIC-IDS2017, 'Protocol' is often the only remaining non-target categorical feature.
            # One-Hot Encoding is appropriate to avoid introducing ordinality (RQ1 justification).
            X = pd.get_dummies(X, columns=categorical_cols, prefix=categorical_cols, drop_first=True)
            print(f"One-Hot Encoded categorical features: {list(categorical_cols)}. New feature count: {X.shape[1]}")
        else:
            print("No non-target categorical features found for One-Hot Encoding.")

        # --- Imputation and Scaling Pipeline ---
        # Choice between SimpleImputer and IterativeImputer (RQ1 justification for advanced imputation)
        # IterativeImputer is more advanced but computationally intensive. SimpleImputer is a robust fallback.
        # For dissertation, explicitly testing IterativeImputer (MICE) is preferred if resources permit.
        # Let's provide an option for IterativeImputer for RQ1's depth.
        try:
            # IterativeImputer is more robust for complex datasets (RQ1 advanced imputation)
            imputer = IterativeImputer(max_iter=5, random_state=random_state, verbose=0)
            print("Using IterativeImputer for advanced missing value imputation.")
        except ImportError:
            print("IterativeImputer not available (requires scikit-learn >= 0.23). Falling back to SimpleImputer(median).")
            imputer = SimpleImputer(strategy='median')
        
        scaler = StandardScaler() # Feature scaling (RQ1)

        # Create a pre-processing pipeline for numerical features
        # Note: We can't put this directly into an ImbPipeline that also includes SMOTETomek
        # because SMOTETomek needs to be applied after splitting, ONLY on training data.
        # This pipeline will be applied to X before splitting.
        numeric_features_pipeline = ImbPipeline([
            ('imputer', imputer),
            ('scaler', scaler)
        ])

        # Fit and transform numerical features
        numeric_cols = X.select_dtypes(include=np.number).columns
        # Ensure column names are preserved
        X[numeric_cols] = pd.DataFrame(numeric_features_pipeline.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)
        print("Applied imputation and feature scaling to numerical features.")

        # --- Encode Target Label ---
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        print(f"Encoded multi-class labels to numeric. Unique encoded labels: {np.unique(y_encoded)}")
        # Store the LabelEncoder for inverse transformation later
        joblib.dump(le, 'label_encoder.pkl')
        print("LabelEncoder saved to 'label_encoder.pkl'")

        return X, pd.Series(y_encoded, name='Label'), le

    # ---
    # STEP 4: DATA SPLIT AND MULTI-CLASS IMBALANCE HANDLING (Phase 2, aligns with RQ1)
    # This step performs the train-test split and applies advanced multi-class imbalance handling.

    def split_and_handle_imbalance(X, y_encoded, test_size=0.2, random_state=42):
        """
        Splits data into training and testing sets, then applies SMOTETomek
        for robust multi-class imbalance handling on the training set.
        """
        print("\n--- STEP 4: Data Split & Multi-Class Imbalance Handling (Phase 2, RQ1) ---")
        print("Splitting data into training and testing sets...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded
        )
        print(f"Original training set shape: {X_train.shape}, Test set shape: {X_test.shape}")
        
        # Display original training label distribution BEFORE resampling
        print("\nOriginal training label distribution (before resampling):")
        original_train_counts = pd.Series(y_train).value_counts().sort_index()
        print(original_train_counts)

        # Use SMOTETomek for robust multi-class oversampling and cleaning (RQ1)
        # 'auto' strategy balances all minority classes to the majority class.
        # Adjusting `smote` and `tomek` parameters could be part of RQ1's "optimized resampling"
        print("Applying SMOTETomek for multi-class imbalance handling on training data... This may take a while. ‚è≥")
        smotetomek = SMOTETomek(random_state=random_state, sampling_strategy='auto', smote=SMOTE(random_state=random_state), tomek=None) # tomek=None to use default
        X_train_res, y_train_res = smotetomek.fit_resample(X_train, y_train)
        
        print(f"Resampled training set shape: {X_train_res.shape}")
        print("\nResampled training label distribution:")
        resampled_train_counts = pd.Series(y_train_res).value_counts().sort_index()
        print(resampled_train_counts)
        print("Multi-class imbalance handling complete! ‚úÖ")

        return X_train_res, X_test, y_train_res, y_test

    # ---
    # STEP 5: FEATURE ENGINEERING / SELECTION (Phase 3, aligns with RQ2)
    # This step applies feature selection using XGBoost's feature importances.
    # A placeholder for more advanced domain-informed feature engineering is also included.

    class FeatureEngineer(object):
        """
        A placeholder class for more advanced, domain-informed feature engineering
        and for performing feature selection using a model's feature importances.
        """
        def __init__(self, n_features_to_select=None, random_state=42):
            self.n_features_to_select = n_features_to_select
            self.selector = None
            self.feature_names_in_ = None
            self.random_state = random_state

        def fit(self, X, y):
            self.feature_names_in_ = X.columns.tolist()
            if self.n_features_to_select is not None and self.n_features_to_select < X.shape[1]:
                print(f"Performing feature selection using SelectFromModel (XGBoost importance) to select top {self.n_features_to_select} features...")
                # Using a simple XGBoost model to get feature importances for selection
                # eval_metric='mlogloss' is more robust for multi-class
                model_for_selection = xgb.XGBClassifier(
                    random_state=self.random_state,
                    n_estimators=100,
                    learning_rate=0.1,
                    use_label_encoder=False,
                    eval_metric='mlogloss',
                    objective='multi:softmax',
                    num_class=len(np.unique(y))
                )
                model_for_selection.fit(X, y)
                self.selector = SelectFromModel(model_for_selection, max_features=self.n_features_to_select, prefit=True)
                # Fit the selector to the model_for_selection (which is already fitted)
                # We need to explicitly fit SelectFromModel on X,y so it gets feature_importances_ and thresholds correctly
                self.selector.fit(X, y)
                print(f"Selected {self.selector.n_features_} features.")
            else:
                print(f"Skipping SelectFromModel as n_features_to_select ({self.n_features_to_select}) is not specified or is >= initial feature count ({X.shape[1]}).")
            return self

        def transform(self, X):
            if self.selector:
                # Get the names of the selected features
                selected_feature_names = X.columns[self.selector.get_support()]
                return pd.DataFrame(self.selector.transform(X), columns=selected_feature_names, index=X.index)
            return X

        def get_feature_names_out(self, input_features=None):
            if self.selector:
                return input_features[self.selector.get_support()] if input_features is not None else self.feature_names_in_[self.selector.get_support()]
            return input_features if input_features is not None else self.feature_names_in_

    def perform_feature_selection(X_train_res, X_test, y_train_res, y_test, num_features=50, random_state=42):
        """
        Applies feature selection based on XGBoost feature importances (RQ2).
        """
        print(f"\n--- STEP 5: Feature Engineering / Selection (Phase 3, RQ2) ---")
        print(f"Initial feature count: {X_train_res.shape[1]}")

        # Placeholder for advanced domain-informed feature engineering (RQ2)
        # Example: X_train_res = add_temporal_features(X_train_res)
        # Example: X_test = add_temporal_features(X_test)
        print("Note: Domain-informed feature engineering (beyond selection) would be implemented here if applicable.")


        fe = FeatureEngineer(n_features_to_select=num_features, random_state=random_state)
        fe.fit(X_train_res, y_train_res) # Fit feature selector only on training data

        X_train_selected = fe.transform(X_train_res)
        X_test_selected = fe.transform(X_test)

        print(f"Features selected: {X_train_selected.shape[1]}")
        print("Feature selection complete! ‚ú®")

        return X_train_selected, X_test_selected


    # ---
    # STEP 6: XGBOOST MODEL TRAINING & HYPERPARAMETER TUNING (Phase 3, aligns with RQ2)
    # This step involves training the optimized XGBoost model with hyperparameter tuning.

    def train_and_tune_xgboost(X_train_selected, y_train_res, le, random_state=42):
        """
        Trains and tunes an XGBoost classifier using RandomizedSearchCV (RQ2).
        """
        print("\n--- STEP 6: XGBoost Model Training & Hyperparameter Tuning (Phase 3, RQ2) ---")
        print("Starting XGBoost model training and hyperparameter tuning... üöÄ")

        # Define XGBoost model (multi-class)
        # use_label_encoder=False is important for newer versions of XGBoost
        # eval_metric='mlogloss' is suitable for multi-class classification
        # objective='multi:softmax' for class labels, 'multi:softprob' for probabilities
        model_xgb = xgb.XGBClassifier(
            objective='multi:softmax',
            num_class=len(le.classes_), # Number of unique classes in target
            eval_metric='mlogloss',
            use_label_encoder=False,
            n_jobs=-1, # Use all available cores
            random_state=random_state
        )

        # Define hyperparameters for RandomizedSearchCV (RQ2)
        # A wide range for exploration, more granular tuning can follow with GridSearchCV
        param_dist = {
            'n_estimators': [100, 200, 300, 500], # Reduced for speed, can increase for thoroughness
            'learning_rate': [0.01, 0.05, 0.1],   # Reduced for speed
            'max_depth': [3, 5, 7, 9],            # Reduced for speed
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
            'gamma': [0, 0.1, 0.2],
            'reg_alpha': [0, 0.001, 0.01], # L1 regularization (RQ2)
            'reg_lambda': [1, 10],         # L2 regularization (RQ2)
            # For multi-class, SMOTETomek handles the balancing.
            # If explicit class weights were needed, they'd be passed to `sample_weight` in .fit()
        }

        # Randomized search for hyperparameter tuning (RQ2)
        # n_iter controls the number of parameter settings that are sampled.
        # cv=3 or 5 for cross-validation folds.
        # scoring='f1_macro' is good for imbalanced multi-class (RQ1, RQ2).
        random_search = RandomizedSearchCV(
            estimator=model_xgb,
            param_distributions=param_dist,
            n_iter=20, # Reduced for speed, increase for more thorough search (e.g., 50-100)
            scoring='f1_macro', # Optimise for macro F1-score (RQ1, RQ2)
            cv=3, # 3-fold cross-validation (can increase to 5 for more robust results)
            verbose=1,
            random_state=random_state,
            n_jobs=-1 # Use all available cores for parallel search
        )

        random_search.fit(X_train_selected, y_train_res)

        best_xgb_model = random_search.best_estimator_
        print(f"\nBest XGBoost parameters found: {random_search.best_params_}")
        print(f"Best cross-validation F1-macro score: {random_search.best_score_:.4f}")
        print("XGBoost training and tuning complete! ‚úÖ")

        return best_xgb_model

    # ---
    # STEP 7: COMPARATIVE ANALYSIS (Phase 3, aligns with RQ3)
    # This step trains and evaluates other state-of-the-art models for comparison.

    def perform_comparative_analysis(X_train_selected, X_test_selected, y_train_res, y_test, le, random_state=42):
        """
        Trains and evaluates other state-of-the-art models for comparative analysis (RQ3).
        """
        print("\n--- STEP 7: Comparative Analysis (Phase 3, RQ3) ---")
        print("\nStarting Comparative Analysis (RQ3)... üìä")

        models = {
            'RandomForest': RandomForestClassifier(random_state=random_state, n_estimators=100, n_jobs=-1),
            'LightGBM': lgb.LGBMClassifier(
                objective='multiclass',
                num_class=len(le.classes_),
                random_state=random_state,
                n_estimators=300, # Example: tuned, can be more extensively tuned
                learning_rate=0.05,
                n_jobs=-1
            ),
            'CatBoost': cb.CatBoostClassifier(
                objective='MultiClass',
                classes_count=len(le.classes_),
                random_state=random_state,
                iterations=300, # Example: tuned
                learning_rate=0.05,
                verbose=0, # Suppress verbose output during training
                thread_count=-1 # Use all available cores
            )
        }

        results = {}
        for name, model in models.items():
            print(f"\nTraining {name}...")
            model.fit(X_train_selected, y_train_res)
            y_pred = model.predict(X_test_selected)
            
            # Calculate F1-score for multi-class
            f1_macro = f1_score(y_test, y_pred, average='macro')
            f1_weighted = f1_score(y_test, y_pred, average='weighted')
            accuracy = accuracy_score(y_test, y_pred)
            
            print(f"\n--- {name} Results ---")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"F1-Macro: {f1_macro:.4f}")
            print(f"F1-Weighted: {f1_weighted:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0)) # zero_division=0 to handle classes with no predicted samples
            
            results[name] = {
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'f1_weighted': f1_weighted,
                'report': classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True, zero_division=0),
                'confusion_matrix': confusion_matrix(y_test, y_pred)
            }
        print("Comparative Analysis Complete! ‚úÖ")
        return results

    # ---
    # Phase 4: Result Analysis
    # This section focuses on evaluating models, visualizing results, and providing interpretability.

    # ---
    # STEP 8: EVALUATION AND INTERPRETABILITY (Phase 4, aligns with RQ1, RQ2, RQ3)
    # This step evaluates the best XGBoost model and provides interpretability insights with SHAP.

    def evaluate_and_interpret(best_xgb_model, comparative_results, X_test_selected, y_test, le, random_state=42):
        """
        Evaluates the best XGBoost model, summarizes comparative results,
        and provides interpretability insights (RQ1, RQ2, RQ3).
        """
        print("\n--- STEP 8: Evaluation & Interpretability (Phase 4, RQ1, RQ2, RQ3) ---")

        # --- Final Optimized XGBoost Model Evaluation (RQ1, RQ2) ---
        print("\n--- Final Optimized XGBoost Model Evaluation ---")
        y_pred_xgb = best_xgb_model.predict(X_test_selected)
        
        # Classification Report
        print("\nClassification Report for Optimized XGBoost:")
        print(classification_report(y_test, y_pred_xgb, target_names=le.classes_, zero_division=0))
        
        # Confusion Matrix
        cm_xgb = confusion_matrix(y_test, y_pred_xgb)
        print("\nConfusion Matrix for Optimized XGBoost:")
        print(cm_xgb)
        
        # Plotting Confusion Matrix
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
        plt.title('Confusion Matrix for Optimized XGBoost (Test Set)')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.tight_layout()
        plt.savefig('confusion_matrix_xgboost.png')
        plt.show()
        print("Confusion matrix plot for Optimized XGBoost saved to 'confusion_matrix_xgoost.png'")

        # --- Summarize Comparative Analysis Results (RQ3) ---
        print("\n--- Summary of Comparative Model Performance (RQ3) ---")
        summary_data = []
        # Add XGBoost results to summary
        f1_xgb_macro = f1_score(y_test, y_pred_xgb, average='macro')
        f1_xgb_weighted = f1_score(y_test, y_pred_xgb, average='weighted')
        acc_xgb = accuracy_score(y_test, y_pred_xgb)
        summary_data.append({'Model': 'XGBoost (Optimized)', 'Accuracy': acc_xgb, 'F1-Macro': f1_xgb_macro, 'F1-Weighted': f1_xgb_weighted})

        for name, res in comparative_results.items():
            summary_data.append({'Model': name, 'Accuracy': res['accuracy'], 'F1-Macro': res['f1_macro'], 'F1-Weighted': res['f1_weighted']})

        summary_df = pd.DataFrame(summary_data).set_index('Model').sort_values(by='F1-Macro', ascending=False)
        print("\nPerformance Summary Table:")
        print(summary_df.to_markdown(numalign="left", stralign="left")) # Nicely formatted table

        # Plotting Summary
        summary_df.plot(kind='bar', figsize=(14, 7))
        plt.title('Comparative Model Performance (F1-Macro on Test Set)')
        plt.ylabel('Score')
        plt.xticks(rotation=45, ha='right')
        plt.legend(loc='lower right')
        plt.tight_layout()
        plt.savefig('comparative_performance_summary.png')
        plt.show()
        print("Comparative performance summary plot saved to 'comparative_performance_summary.png'")


        # --- Feature Importance (RQ2) ---
        print("\n--- XGBoost Feature Importance (RQ2) ---")
        importance_df = pd.DataFrame({
            'Feature': X_test_selected.columns,
            'Importance': best_xgb_model.feature_importances_
        }).sort_values(by='Importance', ascending=False)
        print("Top 10 Most Important Features:")
        print(importance_df.head(10).to_markdown(numalign="left", stralign="left"))

        plt.figure(figsize=(12, 8))
        sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))
        plt.title('Top 20 XGBoost Feature Importances (Optimized Model)')
        plt.tight_layout()
        plt.savefig('xgboost_feature_importance.png')
        plt.show()
        print("Feature importance plot saved to 'xgboost_feature_importance.png'")

        # --- SHAP for Interpretability (RQ2) ---
        print("\n--- SHAP Explanations (RQ2) ---")
        # Using a subset of the test data for SHAP for computational efficiency
        # Adjust as needed, but for multi-class SHAP summary plots, it's often more practical
        # to use a smaller, representative subset of the data for explainer computation.
        print("Generating SHAP plots (this may take a few moments)...")
        shap_sample_X = X_test_selected.sample(n=min(1000, len(X_test_selected)), random_state=random_state)
        
        # Explain all classes for multi-class classification
        explainer = shap.TreeExplainer(best_xgb_model)
        shap_values = explainer.shap_values(shap_sample_X)
        
        if isinstance(shap_values, list): # Multi-class output
            # Plot for each class (optional, or just for key attack classes)
            for i, class_name in enumerate(le.classes_):
                if i == 0 and class_name == 'BENIGN': # Skip benign for attack focus, or plot if needed
                    continue
                if i >= len(shap_values): # Ensure class index is valid
                    continue

                print(f"Plotting SHAP for class: '{class_name}' (Index {i})")
                
                # SHAP Summary Plot (Bar)
                shap.summary_plot(shap_values[i], shap_sample_X, plot_type="bar", show=False)
                plt.title(f"SHAP Feature Importance for '{class_name}' Class")
                plt.tight_layout()
                plt.savefig(f'shap_summary_plot_class_{i}_{class_name}.png')
                plt.show()
                
                # SHAP Beeswarm Plot
                shap.plots.beeswarm(shap_values[i], shap_sample_X, max_display=15, show=False)
                plt.title(f"SHAP Beeswarm Plot for '{class_name}' Class")
                plt.tight_layout()
                plt.savefig(f'shap_beeswarm_plot_class_{i}_{class_name}.png')
                plt.show()
                
                print(f"SHAP plots for class '{class_name}' saved.")
        else: # This path should not be taken for multi-class objective
            print("Warning: SHAP values are not a list, assuming binary. This might indicate an issue with multi-class setup.")
            shap.summary_plot(shap_values, shap_sample_X, show=False)
            plt.tight_layout()
            plt.savefig('shap_summary_plot_binary.png')
            plt.show()
            
            shap.plots.beeswarm(shap_values, shap_sample_X, max_display=15, show=False)
            plt.tight_layout()
            plt.savefig('shap_beeswarm_plot_binary.png')
            plt.show()

        print("Evaluation and Interpretability complete! üìà")
        return best_xgb_model # Return model for potential saving

    # ---
    # MAIN EXECUTION BLOCK

    if __name__ == "__main__":
        DATA_PATH = 'data/CIC-IDS2017' # üîÅ IMPORTANT: Change this to your actual data directory where CSVs are!
        RANDOM_STATE = 42
        N_FEATURES_TO_SELECT = 60 # Example: Select top 60 features (RQ2) - adjust based on EDA/results

        print("--- Initiating Dissertation Pipeline ---")

        # Phase 2: Data Acquisition and Pre-processing
        # Step 2: Load and Initial Clean Data (RQ1)
        df_raw = load_and_initial_clean_data(DATA_PATH)

        # Step 3: Pre-process Data (Imputation, Scaling, Categorical Encoding) (RQ1)
        X_processed, y_encoded, label_encoder = preprocess_data(df_raw, random_state=RANDOM_STATE)

        # Step 4: Split Data and Handle Multi-Class Imbalance (RQ1)
        X_train_resampled, X_test_original, y_train_resampled, y_test_original = \
            split_and_handle_imbalance(X_processed, y_encoded, random_state=RANDOM_STATE)

        # Phase 3: Model Development & Experimentation
        # Step 5: Feature Engineering / Selection (RQ2)
        X_train_final, X_test_final = perform_feature_selection(
            X_train_resampled, X_test_original, y_train_resampled, y_test_original,
            num_features=N_FEATURES_TO_SELECT, random_state=RANDOM_STATE
        )

        # Step 6: XGBoost Model Training & Hyperparameter Tuning (RQ2)
        optimized_xgb_model = train_and_tune_xgboost(X_train_final, y_train_resampled, label_encoder, random_state=RANDOM_STATE)

        # Step 7: Comparative Analysis (RQ3)
        comparative_results = perform_comparative_analysis(
            X_train_final, X_test_final, y_train_resampled, y_test_original, label_encoder, random_state=RANDOM_STATE
        )

        # Phase 4: Result Analysis
        # Step 8: Evaluation and Interpretability (RQ1, RQ2, RQ3)
        # This step uses the optimized XGBoost model for detailed analysis.
        # It also summarizes the comparative results from Step 7.
        evaluate_and_interpret(optimized_xgb_model, comparative_results, X_test_final, y_test_original, label_encoder, random_state=RANDOM_STATE)

        # ---
        # FINAL STEP: SAVE OPTIMIZED XGBOOST MODEL & LABEL ENCODER (for reproducibility)
        joblib.dump(optimized_xgb_model, 'optimized_xgboost_nids_model.pkl')
        print("\nOptimized XGBoost NIDS model saved to 'optimized_xgboost_nids_model.pkl' üíæ")
        # Label encoder is already saved in preprocess_data function.

        print("\nDissertation pipeline execution complete! Good luck with your analysis and writing! üëç")


