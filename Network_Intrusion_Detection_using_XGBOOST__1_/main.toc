\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Chapter 1: Introduction}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Background}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Problem Statement}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Research Questions}{3}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Research Objectives}{3}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Scope of the Dissertation}{3}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Significance of the Research}{4}{subsection.1.6}%
\contentsline {subsection}{\numberline {1.7}Dissertation Outline}{4}{subsection.1.7}%
\contentsline {section}{\numberline {2}Chapter 2: Literature Review}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}Introduction to Network Intrusion Detection System}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}How NIDS detect threats}{5}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Signature-Based Detection}{5}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Anomaly-Based Detection}{6}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Hybrid Detection}{7}{subsubsection.2.2.3}%
\contentsline {subsection}{\numberline {2.3}How NIDS are deployed in the real-world}{7}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Software and Hardware to boost performance}{8}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Rule and Model Management}{8}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Integration with SIEM/SOAR}{8}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Datasets and Evaluation}{9}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Contemporary Challenges}{9}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Encryption and Protocol Evolution}{9}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Performance at Line Rate}{9}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Data and Concept Drift}{10}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}Adversarial Robustness}{10}{subsubsection.2.5.4}%
\contentsline {subsubsection}{\numberline {2.5.5}Explainability and Analyst Trust}{10}{subsubsection.2.5.5}%
\contentsline {subsubsection}{\numberline {2.5.6}Operational Diffculties}{10}{subsubsection.2.5.6}%
\contentsline {subsection}{\numberline {2.6}Future Directions}{10}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}ML in Network Intrusion Detection Systems}{11}{subsection.2.7}%
\contentsline {subsubsection}{\numberline {2.7.1}Introduction}{11}{subsubsection.2.7.1}%
\contentsline {subsection}{\numberline {2.8}Machine Learning Approaches in NIDS}{11}{subsection.2.8}%
\contentsline {subsubsection}{\numberline {2.8.1}Supervised learning}{11}{subsubsection.2.8.1}%
\contentsline {subparagraph}{Strengths and Limitations}{12}{subparagraph*.1}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{12}{paragraph*.2}%
\contentsline {subparagraph}{Application in NIDS}{12}{subparagraph*.3}%
\contentsline {subparagraph}{Strengths and Limitations}{12}{subparagraph*.4}%
\contentsline {paragraph}{Autoencoders}{12}{paragraph*.5}%
\contentsline {subparagraph}{Application in NIDS}{12}{subparagraph*.6}%
\contentsline {subparagraph}{Strengths and Limitations}{13}{subparagraph*.7}%
\contentsline {paragraph}{Variational Autoencoders (VAEs)}{13}{paragraph*.8}%
\contentsline {subparagraph}{Application in NIDS}{13}{subparagraph*.9}%
\contentsline {subparagraph}{Strengths and Limitations}{13}{subparagraph*.10}%
\contentsline {subparagraph}{Strengths and Limitations}{13}{subparagraph*.11}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs) and their Variants (LSTM, GRU)}{13}{paragraph*.12}%
\contentsline {subparagraph}{Application in NIDS}{13}{subparagraph*.13}%
\contentsline {subparagraph}{Strengths and Limitations}{14}{subparagraph*.14}%
\contentsline {paragraph}{Autoencoders (AEs)}{14}{paragraph*.15}%
\contentsline {subparagraph}{Application in NIDS}{14}{subparagraph*.16}%
\contentsline {subparagraph}{Strengths and Limitations}{14}{subparagraph*.17}%
\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{14}{paragraph*.18}%
\contentsline {subparagraph}{Application in NIDS}{14}{subparagraph*.19}%
\contentsline {subparagraph}{Strengths and Limitations}{15}{subparagraph*.20}%
\contentsline {subsubsection}{\numberline {2.8.2}Semi-Supervised Learning}{15}{subsubsection.2.8.2}%
\contentsline {subparagraph}{Application in NIDS}{15}{subparagraph*.21}%
\contentsline {subparagraph}{Strengths and Limitations}{15}{subparagraph*.22}%
\contentsline {subsection}{\numberline {2.9}Challenges and Future Directions of ML in NIDS}{15}{subsection.2.9}%
\contentsline {subsubsection}{\numberline {2.9.1}Data Imbalance}{15}{subsubsection.2.9.1}%
\contentsline {subparagraph}{Mitigation Strategies}{16}{subparagraph*.23}%
\contentsline {subsection}{\numberline {2.10}Exploratory Data Analysis (EDA) of the CIC-IDS2017 Dataset}{16}{subsection.2.10}%
\contentsline {subsubsection}{\numberline {2.10.1}Brief Introduction}{16}{subsubsection.2.10.1}%
\contentsline {subsubsection}{\numberline {2.10.2}The CIC-IDS2017 Dataset: Background and Key Characteristics}{17}{subsubsection.2.10.2}%
\contentsline {subsubsection}{\numberline {2.10.3}Structure and Features}{18}{subsubsection.2.10.3}%
\contentsline {subsubsection}{\numberline {2.10.4}Dataset Overview}{19}{subsubsection.2.10.4}%
\contentsline {subsection}{\numberline {2.11}Preliminary EDA: Initial Impressions and Red Flags}{25}{subsection.2.11}%
\contentsline {subsection}{\numberline {2.12}Dataset Summary Statistics}{26}{subsection.2.12}%
\contentsline {subsection}{\numberline {2.13}Dataset Characteristics and Features}{26}{subsection.2.13}%
\contentsline {subsection}{\numberline {2.14}Initial Data Exploration (EDA)}{27}{subsection.2.14}%
\contentsline {subsubsection}{\numberline {2.14.1}Loading and Merging Data}{27}{subsubsection.2.14.1}%
\contentsline {subsubsection}{\numberline {2.14.2}Data Types and Initial Statistics}{28}{subsubsection.2.14.2}%
\contentsline {subsubsection}{\numberline {2.14.3}NaN (Missing Values) Treatment}{28}{subsubsection.2.14.3}%
\contentsline {subsubsection}{\numberline {2.14.4}Inf (Infinite Values) Treatment}{29}{subsubsection.2.14.4}%
\contentsline {subsubsection}{\numberline {2.14.5}Encoding Categorical Features}{29}{subsubsection.2.14.5}%
\contentsline {subsubsection}{\numberline {2.14.6}Analysis of Class Distribution}{30}{subsubsection.2.14.6}%
\contentsline {subsubsection}{\numberline {2.14.7}Feature-Wise Analysis and Outlier Detection}{30}{subsubsection.2.14.7}%
\contentsline {subsubsection}{\numberline {2.14.8}Correlation Analysis}{31}{subsubsection.2.14.8}%
\contentsline {subsubsection}{\numberline {2.14.9}Identified Challenges and their Implications for the Methodology}{32}{subsubsection.2.14.9}%
\contentsline {paragraph}{1. Severe Class Imbalance}{32}{paragraph*.36}%
\contentsline {paragraph}{2. Missing and Infinite Values}{32}{paragraph*.37}%
\contentsline {paragraph}{3. Redundant and Highly Correlated Features}{32}{paragraph*.38}%
\contentsline {paragraph}{4. Presence of Outliers}{33}{paragraph*.39}%
\contentsline {paragraph}{5. Categorical Feature Encoding}{33}{paragraph*.40}%
\contentsline {subsubsection}{\numberline {2.14.10}Addressing EDA Insights in the Methodology}{33}{subsubsection.2.14.10}%
\contentsline {subsubsection}{\numberline {2.14.11}Conclusion of Dataset Exploration}{35}{subsubsection.2.14.11}%
\contentsline {subsection}{\numberline {2.15}Deep Dive into Ensemble Learning}{35}{subsection.2.15}%
\contentsline {subsubsection}{\numberline {2.15.1}Introduction}{35}{subsubsection.2.15.1}%
\contentsline {subsubsection}{\numberline {2.15.2}The Basics: Foundations of Ensemble Learning}{36}{subsubsection.2.15.2}%
\contentsline {subsubsection}{\numberline {2.15.3}Bias-Variance Trade-off}{36}{subsubsection.2.15.3}%
\contentsline {subsubsection}{\numberline {2.15.4}Bagging (Bootstrap Aggregating)}{37}{subsubsection.2.15.4}%
\contentsline {subsubsection}{\numberline {2.15.5}Bootstrapping}{37}{subsubsection.2.15.5}%
\contentsline {subsubsection}{\numberline {2.15.6}Workings of Bagging}{37}{subsubsection.2.15.6}%
\contentsline {subsubsection}{\numberline {2.15.7}Random Forest (RF)}{38}{subsubsection.2.15.7}%
\contentsline {paragraph}{A Closer Look}{38}{paragraph*.41}%
\contentsline {paragraph}{RF in the NIDS: Strengths}{38}{paragraph*.42}%
\contentsline {paragraph}{Limitations}{39}{paragraph*.43}%
\contentsline {paragraph}{Applications and Impact in NIDS}{39}{paragraph*.44}%
\contentsline {subsubsection}{\numberline {2.15.8}Boosting}{39}{subsubsection.2.15.8}%
\contentsline {subsubsection}{\numberline {2.15.9}The General Principle of Gradient Boosting Machines (GBM)}{39}{subsubsection.2.15.9}%
\contentsline {subsubsection}{\numberline {2.15.10}Extreme Gradient Boosting (XGBoost)}{40}{subsubsection.2.15.10}%
\contentsline {paragraph}{Inside the Black Box}{40}{paragraph*.45}%
\contentsline {paragraph}{Key Advantages}{41}{paragraph*.46}%
\contentsline {subsubsection}{\numberline {2.15.11}Computational Complexity and Resource Requirements}{41}{subsubsection.2.15.11}%
\contentsline {subsubsection}{\numberline {2.15.12}Model Complexity and Interpretability}{41}{subsubsection.2.15.12}%
\contentsline {subsubsection}{\numberline {2.15.13}Integration with Existing Infrastructure}{42}{subsubsection.2.15.13}%
\contentsline {subsubsection}{\numberline {2.15.14}Overfitting with Improper Tuning}{42}{subsubsection.2.15.14}%
\contentsline {subsubsection}{\numberline {2.15.15}Maintaining Performance with Evolving Threats}{42}{subsubsection.2.15.15}%
\contentsline {subsubsection}{\numberline {2.15.16}Dataset Specificity and Transferability}{42}{subsubsection.2.15.16}%
\contentsline {subsubsection}{\numberline {2.15.17}Hyperparameter Tuning and Configuration}{43}{subsubsection.2.15.17}%
\contentsline {subsubsection}{\numberline {2.15.18}Increased Computational Cost and Training Time}{43}{subsubsection.2.15.18}%
\contentsline {subsubsection}{\numberline {2.15.19}Reduced Interpretability (Black-Box Nature)}{43}{subsubsection.2.15.19}%
\contentsline {subsubsection}{\numberline {2.15.20}Hyperparameter Complexity}{43}{subsubsection.2.15.20}%
\contentsline {subsubsection}{\numberline {2.15.21}Vulnerability to Adversarial Attacks}{43}{subsubsection.2.15.21}%
\contentsline {subsection}{\numberline {2.16}Conclusion}{44}{subsection.2.16}%
\contentsline {subsection}{\numberline {2.17}Deep Dive into XGBoost: Principles, Advantages, and Hyperparameters}{44}{subsection.2.17}%
\contentsline {subsubsection}{\numberline {2.17.1}Introduction to XGBoost}{44}{subsubsection.2.17.1}%
\contentsline {subsubsection}{\numberline {2.17.2}Core Principles of XGBoost}{45}{subsubsection.2.17.2}%
\contentsline {subsubsection}{\numberline {2.17.3}Ensemble Learning and Boosting}{45}{subsubsection.2.17.3}%
\contentsline {subsubsection}{\numberline {2.17.4}Gradient Boosting Machines (GBM)}{45}{subsubsection.2.17.4}%
\contentsline {subsubsection}{\numberline {2.17.5}Decision Trees as Weak Learners}{46}{subsubsection.2.17.5}%
\contentsline {subsubsection}{\numberline {2.17.6}XGBoost: Algorithmic Enhancements and Mechanics}{46}{subsubsection.2.17.6}%
\contentsline {subsubsection}{\numberline {2.17.7}Regularization in the Objective Function}{47}{subsubsection.2.17.7}%
\contentsline {subsubsection}{\numberline {2.17.8}Sparsity-Aware Split Finding}{47}{subsubsection.2.17.8}%
\contentsline {subsubsection}{\numberline {2.17.9}Approximate Greedy Algorithm and Weighted Quantile Sketch}{48}{subsubsection.2.17.9}%
\contentsline {subsubsection}{\numberline {2.17.10}Shrinkage (Learning Rate)}{48}{subsubsection.2.17.10}%
\contentsline {subsubsection}{\numberline {2.17.11}Column Subsampling (Feature Bagging)}{48}{subsubsection.2.17.11}%
\contentsline {subsubsection}{\numberline {2.17.12}Tree Pruning}{49}{subsubsection.2.17.12}%
\contentsline {subsubsection}{\numberline {2.17.13}Parallel Computing}{49}{subsubsection.2.17.13}%
\contentsline {subsubsection}{\numberline {2.17.14}Advantages of XGBoost}{49}{subsubsection.2.17.14}%
\contentsline {subsubsection}{\numberline {2.17.15}Superior Accuracy}{49}{subsubsection.2.17.15}%
\contentsline {subsubsection}{\numberline {2.17.16}High Speed and Scalability}{49}{subsubsection.2.17.16}%
\contentsline {subsubsection}{\numberline {2.17.17}Robustness Against Overfitting}{50}{subsubsection.2.17.17}%
\contentsline {subsubsection}{\numberline {2.17.18}Effective Handling of Missing Values}{50}{subsubsection.2.17.18}%
\contentsline {subsubsection}{\numberline {2.17.19}Flexibility and Customization}{50}{subsubsection.2.17.19}%
\contentsline {subsubsection}{\numberline {2.17.20}Built-in Feature Importance}{50}{subsubsection.2.17.20}%
\contentsline {subsubsection}{\numberline {2.17.21}Cross-Platform Compatibility}{51}{subsubsection.2.17.21}%
\contentsline {subsubsection}{\numberline {2.17.22}Key Hyperparameters and Their Impact}{51}{subsubsection.2.17.22}%
\contentsline {subsubsection}{\numberline {2.17.23}General Parameters}{51}{subsubsection.2.17.23}%
\contentsline {subsubsection}{\numberline {2.17.24}Booster Parameters (Tree-Specific Parameters for gbtree )}{51}{subsubsection.2.17.24}%
\contentsline {subsubsection}{\numberline {2.17.25}Learning Task Parameters}{53}{subsubsection.2.17.25}%
\contentsline {subsubsection}{\numberline {2.17.26}Tuning Strategy Implications for NIDS}{54}{subsubsection.2.17.26}%
\contentsline {subsubsection}{\numberline {2.17.27}XGBoost in Network Intrusion Detection (NIDS)}{55}{subsubsection.2.17.27}%
\contentsline {subsection}{\numberline {2.18}Identification of Research Gaps}{56}{subsection.2.18}%
\contentsline {subsubsection}{\numberline {2.18.1}Pinpointing Specific Research Gaps for Contribution}{56}{subsubsection.2.18.1}%
\contentsline {subsubsection}{\numberline {2.18.2}Novel Pre-processing Techniques, particularly Adaptive Imputation for Multi-Class}{56}{subsubsection.2.18.2}%
\contentsline {subsubsection}{\numberline {2.18.3}Optimized XGBoost Hyperparameter Tuning with Multi-Class Imbalance in Mind}{57}{subsubsection.2.18.3}%
\contentsline {section}{\numberline {3}Chapter 3: Methodology}{60}{section.3}%
\contentsline {subsection}{\numberline {3.1}Introduction}{60}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Dataset Description: CIC-IDS2017}{60}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Data Pre-processing}{60}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Data Loading and Initial Inspection}{60}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Handling Missing and Infinite Values}{60}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Handling Categorical Features}{61}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Feature Scaling/Normalization}{61}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Feature Engineering (if applicable)}{61}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Handling Class Imbalance}{61}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Model Selection}{61}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Experimental Setup}{62}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Data Splitting}{62}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Evaluation Metrics}{62}{subsubsection.3.11.2}%
\contentsline {subsection}{\numberline {3.12}Tools and Environment}{63}{subsection.3.12}%
\contentsline {subsection}{\numberline {3.13}Hyperparameter Tuning}{63}{subsection.3.13}%
\contentsline {subsection}{\numberline {3.14}Comparative Analysis (if applicable)}{63}{subsection.3.14}%
\contentsline {subsection}{\numberline {3.15}layout}{64}{subsection.3.15}%
\contentsline {subsection}{\numberline {3.16}Ethical Considerations}{64}{subsection.3.16}%
\contentsline {section}{\numberline {4}Chapter 4: Results}{65}{section.4}%
\contentsline {subsection}{\numberline {4.1}Introduction}{65}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Dataset Characteristics after Pre-processing}{65}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Baseline Model Performance}{65}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Hyperparameter Tuning Results}{66}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Optimized XGBoost Model Performance}{66}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Comparative Analysis Results}{70}{subsection.4.6}%
\contentsline {section}{\numberline {5}Chapter 5: Discussion}{72}{section.5}%
\contentsline {subsection}{\numberline {5.1}Interpretation of Results}{72}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Answering Research Questions}{73}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Comparison with Related Work}{73}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Limitations of the Study}{74}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Implications and Contributions}{74}{subsection.5.5}%
\contentsline {section}{\numberline {6}Chapter 6: Conclusion and Future Work}{75}{section.6}%
\contentsline {subsection}{\numberline {6.1}Conclusion}{75}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Future Work}{75}{subsection.6.2}%
\contentsline {section}{\numberline {7}Ethics}{77}{section.7}%
\contentsline {section}{\numberline {8}Appendix}{I}{section.8}%
